{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_process_drain3 import process\n",
    "import csv\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_csv = '../dataset/otlp/exported_data_1114-1120.csv'\n",
    "output_dir = '../dataset/otlp_f1/'\n",
    "export_data_dir = output_dir + 'export_data/'\n",
    "label_dir = output_dir + 'label/'\n",
    "result_dir = '../output/otlp_parser_config/model/result_v5/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openfile(input_filename):\n",
    "    with open(input_filename, mode='r', newline='', encoding='utf-8') as infile:\n",
    "        reader = csv.DictReader(infile)\n",
    "        rows = list(reader)\n",
    "    return reader, rows\n",
    "\n",
    "def tracegroup(rows):\n",
    "    trace_groups = {}\n",
    "    for row in rows:\n",
    "        trace_id = row['TraceId']\n",
    "        if trace_id not in trace_groups:\n",
    "            trace_groups[trace_id] = []\n",
    "        trace_groups[trace_id].append(row)\n",
    "    return trace_groups\n",
    "\n",
    "def outputfile(reader, trace_groups, output_filename):\n",
    "    with open(output_filename, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "        writer = csv.DictWriter(outfile, fieldnames=reader.fieldnames)\n",
    "        writer.writeheader()\n",
    "        for trace_group in trace_groups.values():\n",
    "            for row in trace_group:\n",
    "                writer.writerow(row)\n",
    "                \n",
    "def output_label(trace_groups, modified_trace_ids, filename):\n",
    "    with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=['TraceId', 'is_anomaly'])\n",
    "        writer.writeheader()\n",
    "        for trace_id in trace_groups.keys():\n",
    "            is_modified = 'True' if trace_id in modified_trace_ids else 'False'\n",
    "            writer.writerow({'TraceId': trace_id, 'is_anomaly': is_modified})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1392854/3132479331.py:5: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  selected_trace_ids = random.sample(trace_groups.keys(), k=int(len(trace_groups) * 0.05))\n"
     ]
    }
   ],
   "source": [
    "reader, rows = openfile(input_csv)\n",
    "trace_groups = tracegroup(rows)\n",
    "\n",
    "# 隨機選擇5%的trace進行處理\n",
    "selected_trace_ids = random.sample(trace_groups.keys(), k=int(len(trace_groups) * 0.05))\n",
    "\n",
    "# 處理每個trace，刪除Timestamp最晚的3個資料\n",
    "for trace_id, traces in trace_groups.items():\n",
    "    if trace_id in selected_trace_ids:\n",
    "        traces.sort(key=lambda x: x['Timestamp'], reverse=True)  # 根據Timestamp降序排列\n",
    "        del traces[:3]  # 刪除最晚的3個資料\n",
    "\n",
    "output_csv = export_data_dir + 'exported_data_remove3.csv'\n",
    "label_csv = label_dir + 'label_remove3.csv'\n",
    "\n",
    "outputfile(reader, trace_groups, output_csv)\n",
    "\n",
    "# 輸出被修改過的trace記錄\n",
    "output_label(trace_groups, selected_trace_ids, label_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_csv(input_filename, output_filename):\n",
    "    \n",
    "    reader, rows = openfile(input_filename)\n",
    "    trace_groups = tracegroup(rows)\n",
    "\n",
    "    # 處理每個trace，刪除Timestamp最晚的一半資料\n",
    "    for trace_id, traces in trace_groups.items():\n",
    "        traces.sort(key=lambda x: x['Timestamp'], reverse=True)  # 根據Timestamp降序排列\n",
    "        num_to_remove = len(traces) // 2  # 決定刪除一半的資料\n",
    "        del traces[:num_to_remove]  # 刪除最晚的一半資料\n",
    "\n",
    "    outputfile(reader, trace_groups, output_filename)\n",
    "\n",
    "output_csv = output_dir + 'exported_data_removed.csv'\n",
    "process_csv(input_csv, output_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_csv(input_filename, output_filename):\n",
    "    reader, rows = openfile(input_filename)\n",
    "    trace_groups = tracegroup(rows)\n",
    "\n",
    "    # 處理每個trace\n",
    "    for trace_id, grouped_rows in trace_groups.items():\n",
    "        # 找出所有Content不同的數據\n",
    "        unique_contents = list({row['Content'] for row in grouped_rows})\n",
    "        if len(unique_contents) > 1:\n",
    "            # 隨機選擇兩筆Content不同的數據\n",
    "            content_to_swap = random.sample(unique_contents, 2)\n",
    "            rows_to_swap = [row for row in grouped_rows if row['Content'] in content_to_swap]\n",
    "\n",
    "            # 如果找到兩筆資料，則交換它們的Content\n",
    "            if len(rows_to_swap) == 2:\n",
    "                rows_to_swap[0]['Content'], rows_to_swap[1]['Content'] = rows_to_swap[1]['Content'], rows_to_swap[0]['Content']\n",
    "\n",
    "    outputfile(reader, trace_groups, output_filename)\n",
    "\n",
    "output_csv = output_dir + 'exported_data_switch.csv'\n",
    "process_csv(input_csv, output_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2290147/336772980.py:5: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  selected_trace_ids = random.sample(trace_groups.keys(), k=int(len(trace_groups) * 0.05))\n"
     ]
    }
   ],
   "source": [
    "reader, rows = openfile(input_csv)\n",
    "trace_groups = tracegroup(rows)\n",
    "\n",
    "# 隨機選擇5%的trace進行處理\n",
    "selected_trace_ids = random.sample(trace_groups.keys(), k=int(len(trace_groups) * 0.05))\n",
    "\n",
    "# 處理每個trace\n",
    "for trace_id, grouped_rows in trace_groups.items():\n",
    "    if trace_id in selected_trace_ids:\n",
    "        # 根據Timestamp降序排列找到最後一筆數據\n",
    "        grouped_rows.sort(key=lambda x: x['Timestamp'], reverse=True)\n",
    "        last_row = grouped_rows[0]\n",
    "        # 複製最後一筆數據一百次\n",
    "        for _ in range(100):\n",
    "            grouped_rows.append(last_row.copy())  # 使用copy以避免引用同一個字典對象\n",
    "        \n",
    "output_csv = export_data_dir + 'exported_data_tail100.csv'\n",
    "label_csv = label_dir + 'label_tail100.csv'\n",
    "\n",
    "outputfile(reader, trace_groups, output_csv)\n",
    "\n",
    "# 輸出被修改過的trace記錄\n",
    "output_label(trace_groups, selected_trace_ids, label_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1392854/1693205197.py:5: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  selected_trace_ids = random.sample(trace_groups.keys(), k=int(len(trace_groups) * 0.05))\n"
     ]
    }
   ],
   "source": [
    "reader, rows = openfile(input_csv)\n",
    "trace_groups = tracegroup(rows)\n",
    "\n",
    "# 隨機選擇5%的trace進行處理\n",
    "selected_trace_ids = random.sample(trace_groups.keys(), k=int(len(trace_groups) * 0.05))\n",
    "\n",
    "# 處理每個trace\n",
    "for trace_id, grouped_rows in trace_groups.items():\n",
    "    if trace_id in selected_trace_ids:\n",
    "        # 篩選出Content開頭不是\"Select...\"的數據\n",
    "        non_select_rows = [row for row in grouped_rows if not row['Content'].startswith(\"Select\")]\n",
    "        if non_select_rows:\n",
    "            # 從符合條件的數據中隨機選擇一筆數據\n",
    "            selected_row = random.choice(non_select_rows)\n",
    "            # 複製選定的數據一百次\n",
    "            for _ in range(100):\n",
    "                grouped_rows.append(selected_row.copy())  # 使用copy以避免引用同一個字典對象\n",
    "\n",
    "output_csv = export_data_dir + 'exported_data_random100.csv'\n",
    "label_csv = label_dir + 'label_random100.csv'\n",
    "\n",
    "outputfile(reader, trace_groups, output_csv)\n",
    "\n",
    "# 輸出被修改過的trace記錄\n",
    "output_label(trace_groups, selected_trace_ids, label_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1392854/2906096968.py:5: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  selected_trace_ids = random.sample(trace_groups.keys(), k=int(len(trace_groups) * 0.05))\n"
     ]
    }
   ],
   "source": [
    "reader, rows = openfile(input_csv)\n",
    "trace_groups = tracegroup(rows)\n",
    "\n",
    "# 隨機選擇5%的trace進行處理\n",
    "selected_trace_ids = random.sample(trace_groups.keys(), k=int(len(trace_groups) * 0.05))\n",
    "\n",
    "# 處理每個trace\n",
    "for trace_id, grouped_rows in trace_groups.items():\n",
    "    if trace_id in selected_trace_ids:\n",
    "        # 根據Timestamp降序排列\n",
    "        grouped_rows.sort(key=lambda x: x['Timestamp'], reverse=True)\n",
    "        \n",
    "        # 找出不以\"Select...\"開頭的連續兩筆數據\n",
    "        pair_indices = [(i, i+1) for i in range(len(grouped_rows)-1) \n",
    "                        if not (grouped_rows[i]['Content'].startswith(\"Select\") and \n",
    "                                grouped_rows[i+1]['Content'].startswith(\"Select\"))]\n",
    "        \n",
    "        if pair_indices:\n",
    "            # 隨機選擇一對連續的數據\n",
    "            selected_pair_index = random.choice(pair_indices)\n",
    "            selected_rows = grouped_rows[selected_pair_index[0]:selected_pair_index[1]+1]\n",
    "            \n",
    "            # 複製選定的數據一百次\n",
    "            for _ in range(100):\n",
    "                grouped_rows.extend([row.copy() for row in selected_rows])\n",
    "\n",
    "output_csv = export_data_dir + 'exported_data_pair100.csv'\n",
    "label_csv = label_dir + 'label_pair100.csv'\n",
    "\n",
    "outputfile(reader, trace_groups, output_csv)\n",
    "\n",
    "# 輸出被修改過的trace記錄\n",
    "output_label(trace_groups, selected_trace_ids, label_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_csv(input_filename, output_filename):\n",
    "    reader, rows = openfile(input_filename)\n",
    "    trace_groups = tracegroup(rows)\n",
    "\n",
    "    # 處理每個trace，隨機打亂順序\n",
    "    for trace_id, grouped_rows in trace_groups.items():\n",
    "        random.shuffle(grouped_rows)\n",
    "                \n",
    "    outputfile(reader, trace_groups, output_filename)\n",
    "\n",
    "output_csv = output_dir + 'exported_data_shuffle.csv'\n",
    "process_csv(input_csv, output_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_csv(input_filename, output_filename):\n",
    "    reader, rows = openfile(input_filename)\n",
    "    trace_groups = tracegroup(rows)\n",
    "\n",
    "    # 處理每個trace，刪除所有\"Content\"開頭為\"Select\"的數據\n",
    "    for trace_id, grouped_rows in trace_groups.items():\n",
    "        # 篩選出\"Content\"開頭不是\"Select\"的數據\n",
    "        trace_groups[trace_id] = [row for row in grouped_rows if not row['Content'].startswith(\"Select\")]\n",
    "                    \n",
    "    outputfile(reader, trace_groups, output_filename)\n",
    "\n",
    "output_csv = output_dir + 'exported_data_deleteDatabase.csv'\n",
    "process_csv(input_csv, output_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_csv(input_filename, output_filename):\n",
    "    reader, rows = openfile(input_filename)\n",
    "    trace_groups = tracegroup(rows)\n",
    "\n",
    "    # 處理每個trace，刪除所有\"Content\"開頭為\"Select\"的數據\n",
    "    for trace_id, rows in trace_groups.items():\n",
    "        # 計算要插入的 \"Error\" 數據的數量，為當前 trace 數量的 30%\n",
    "        insert_count = int(len(rows) * 0.4)\n",
    "\n",
    "        # 對每個要插入的數據\n",
    "        for _ in range(insert_count):\n",
    "            # 隨機選擇一筆現有的數據來複製其值（除了 Content）\n",
    "            template_row = random.choice(rows)\n",
    "            new_row = template_row.copy()\n",
    "            \n",
    "            # 將 \"Content\" 設置為 \"Error\"\n",
    "            new_row['Content'] = \"Error\"\n",
    "            \n",
    "            # 將這筆新數據插入到 rows 列表中的隨機位置\n",
    "            insert_position = random.randint(0, len(rows))\n",
    "            rows.insert(insert_position, new_row)\n",
    "\n",
    "    outputfile(reader, trace_groups, output_filename)\n",
    "\n",
    "output_csv = output_dir + 'exported_data_insert.csv'\n",
    "process_csv(input_csv, output_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2290147/1489948926.py:5: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  selected_trace_ids = random.sample(trace_groups.keys(), k=int(len(trace_groups) * 0.05))\n"
     ]
    }
   ],
   "source": [
    "reader, rows = openfile(input_csv)\n",
    "trace_groups = tracegroup(rows)\n",
    "\n",
    "# 隨機選擇5%的trace進行處理\n",
    "selected_trace_ids = random.sample(trace_groups.keys(), k=int(len(trace_groups) * 0.05))\n",
    "\n",
    "# 處理每個trace\n",
    "for trace_id, grouped_rows in trace_groups.items():\n",
    "    if trace_id in selected_trace_ids:\n",
    "        # 篩選出開頭不為\"Select\"的數據\n",
    "        non_select_rows = [row for row in grouped_rows if not row['Content'].startswith(\"Select\")]\n",
    "        if non_select_rows:\n",
    "            # 隨機選擇一筆數據\n",
    "            selected_row = random.choice(non_select_rows)\n",
    "            selected_content = selected_row['Content']\n",
    "            # 將選定的Content替換該組中的其他所有數據的Content\n",
    "            for row in grouped_rows:\n",
    "                row['Content'] = selected_content\n",
    "\n",
    "output_csv = export_data_dir + 'exported_data_replace.csv'\n",
    "label_csv = label_dir + 'label_replace.csv'\n",
    "\n",
    "outputfile(reader, trace_groups, output_csv)\n",
    "\n",
    "# 輸出被修改過的trace記錄\n",
    "output_label(trace_groups, selected_trace_ids, label_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1392854/3239014989.py:5: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  selected_trace_ids = random.sample(trace_groups.keys(), k=int(len(trace_groups) * 0.05))\n"
     ]
    }
   ],
   "source": [
    "reader, rows = openfile(input_csv)\n",
    "trace_groups = tracegroup(rows)\n",
    "\n",
    "# 隨機選擇5%的trace進行處理\n",
    "selected_trace_ids = random.sample(trace_groups.keys(), k=int(len(trace_groups) * 0.05))\n",
    "\n",
    "# 處理每個trace\n",
    "for trace_id, grouped_rows in trace_groups.items():\n",
    "    if trace_id in selected_trace_ids:\n",
    "        # 篩選出Content不同的數據\n",
    "        unique_contents = list({row['Content'] for row in grouped_rows})\n",
    "        \n",
    "        if len(unique_contents) >= 2:\n",
    "            # 隨機選擇兩個不同的Content\n",
    "            selected_contents = random.sample(unique_contents, 2)\n",
    "            \n",
    "            # 分別找到這兩個Content對應的數據\n",
    "            selected_rows = [next(row for row in grouped_rows if row['Content'] == content) for content in selected_contents]\n",
    "            \n",
    "            # 清空原分組數據，並加入新的數據\n",
    "            trace_groups[trace_id] = []\n",
    "            for _ in range(50):\n",
    "                trace_groups[trace_id].append(selected_rows[0].copy())\n",
    "            for _ in range(50):\n",
    "                trace_groups[trace_id].append(selected_rows[1].copy())\n",
    "\n",
    "output_csv = export_data_dir + 'exported_data_5050.csv'\n",
    "label_csv = label_dir + 'label_5050.csv'\n",
    "\n",
    "outputfile(reader, trace_groups, output_csv)\n",
    "\n",
    "# 輸出被修改過的trace記錄\n",
    "output_label(trace_groups, selected_trace_ids, label_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2290147/2482199126.py:5: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  selected_trace_ids = random.sample(trace_groups.keys(), k=int(len(trace_groups) * 0.05))\n"
     ]
    }
   ],
   "source": [
    "reader, rows = openfile(input_csv)\n",
    "trace_groups = tracegroup(rows)\n",
    "\n",
    "# 隨機選擇5%的trace進行處理\n",
    "selected_trace_ids = random.sample(trace_groups.keys(), k=int(len(trace_groups) * 0.05))\n",
    "\n",
    "# 處理每個trace\n",
    "for trace_id, grouped_rows in trace_groups.items():\n",
    "    if trace_id in selected_trace_ids:\n",
    "        # 篩選出Content不同的數據\n",
    "        unique_contents = list({row['Content'] for row in grouped_rows})\n",
    "        \n",
    "        if len(unique_contents) >= 3:\n",
    "            # 隨機選擇兩個不同的Content\n",
    "            selected_contents = random.sample(unique_contents, 3)\n",
    "            \n",
    "            # 分別找到這兩個Content對應的數據\n",
    "            selected_rows = [next(row for row in grouped_rows if row['Content'] == content) for content in selected_contents]\n",
    "            \n",
    "            # 清空原分組數據，並加入新的數據\n",
    "            trace_groups[trace_id] = []\n",
    "            for _ in range(30):\n",
    "                trace_groups[trace_id].append(selected_rows[0].copy())\n",
    "            for _ in range(30):\n",
    "                trace_groups[trace_id].append(selected_rows[1].copy())\n",
    "            for _ in range(30):\n",
    "                trace_groups[trace_id].append(selected_rows[2].copy())\n",
    "\n",
    "output_csv = export_data_dir + 'exported_data_303030.csv'\n",
    "label_csv = label_dir + 'label_303030.csv'\n",
    "\n",
    "outputfile(reader, trace_groups, output_csv)\n",
    "\n",
    "# 輸出被修改過的trace記錄\n",
    "output_label(trace_groups, selected_trace_ids, label_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2290147/587186319.py:5: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  selected_trace_ids = random.sample(trace_groups.keys(), k=int(len(trace_groups) * 0.05))\n"
     ]
    }
   ],
   "source": [
    "reader, rows = openfile(input_csv)\n",
    "trace_groups = tracegroup(rows)\n",
    "\n",
    "# 隨機選擇5%的trace進行處理\n",
    "selected_trace_ids = random.sample(trace_groups.keys(), k=int(len(trace_groups) * 0.05))\n",
    "\n",
    "# 處理每個trace\n",
    "for trace_id, grouped_rows in trace_groups.items():\n",
    "    if trace_id in selected_trace_ids:\n",
    "        repeated_trace = grouped_rows * 10  # 將分組中的數據循環10次\n",
    "        trace_groups[trace_id] = repeated_trace\n",
    "\n",
    "output_csv = export_data_dir + 'exported_data_loop10.csv'\n",
    "label_csv = label_dir + 'label_loop10.csv'\n",
    "\n",
    "outputfile(reader, trace_groups, output_csv)\n",
    "\n",
    "# 輸出被修改過的trace記錄\n",
    "output_label(trace_groups, selected_trace_ids, label_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2290147/315324311.py:5: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  selected_trace_ids = random.sample(trace_groups.keys(), k=int(len(trace_groups) * 0.05))\n"
     ]
    }
   ],
   "source": [
    "reader, rows = openfile(input_csv)\n",
    "trace_groups = tracegroup(rows)\n",
    "\n",
    "# 隨機選擇5%的trace進行處理\n",
    "selected_trace_ids = random.sample(trace_groups.keys(), k=int(len(trace_groups) * 0.05))\n",
    "\n",
    "# 處理每個trace\n",
    "for trace_id, grouped_rows in trace_groups.items():\n",
    "    if trace_id in selected_trace_ids:\n",
    "        # 創建一個新的列表來存儲複製後的數據\n",
    "        duplicated_rows = []\n",
    "\n",
    "        for row in grouped_rows:\n",
    "            # 對每筆數據進行10次複製\n",
    "            duplicated_rows.extend([row.copy() for _ in range(10)])\n",
    "\n",
    "        # 更新該TraceId分組的數據\n",
    "        trace_groups[trace_id] = duplicated_rows\n",
    "        \n",
    "output_csv = export_data_dir + 'exported_data_multi10.csv'\n",
    "label_csv = label_dir + 'label_multi10.csv'\n",
    "\n",
    "outputfile(reader, trace_groups, output_csv)\n",
    "\n",
    "# 輸出被修改過的trace記錄\n",
    "output_label(trace_groups, selected_trace_ids, label_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: result_df_tail100.csv has 39 anomalies.\n",
      "File: result_df_5050.csv has 68 anomalies.\n",
      "File: result_df_replace.csv has 68 anomalies.\n",
      "File: result_df_random100.csv has 67 anomalies.\n",
      "File: result_df_303030.csv has 67 anomalies.\n",
      "File: result_df_1114-1120.csv has 8 anomalies.\n",
      "File: result_df_loop10.csv has 45 anomalies.\n",
      "File: result_df_multi10.csv has 51 anomalies.\n",
      "File: result_df_pair100.csv has 67 anomalies.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "directory_path = result_dir\n",
    "\n",
    "for filename in os.listdir(directory_path):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        \n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        anomaly_count = df[df['is_anomaly'] == True].shape[0]\n",
    "        \n",
    "        print(f\"File: {filename} has {anomaly_count} anomalies.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀取異常標記文件\n",
    "anomaly_df = pd.read_csv('../output/otlp_parser_config/model/result_v4/result_df_1114-1120.csv')\n",
    "\n",
    "# 獲取所有被標記為異常的TraceId\n",
    "anomaly_trace_ids = set(anomaly_df[anomaly_df['is_anomaly'] == True]['TraceId'])\n",
    "\n",
    "# 讀取原始trace數據\n",
    "exported_data_df = pd.read_csv('../dataset/otlp/exported_data_1114-1120.csv')\n",
    "\n",
    "# 移除被標記為異常的trace\n",
    "exported_data_df = exported_data_df[~exported_data_df['TraceId'].isin(anomaly_trace_ids)]\n",
    "\n",
    "# 保存處理後的數據\n",
    "exported_data_df.to_csv('../dataset/otlp/exported_data_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: label_303030.csv result_df_303030.csv | Recall: 1.0, Precision: 0.8955223880597015, F1-Score: 0.9448818897637796\n",
      "File: label_5050.csv result_df_5050.csv | Recall: 0.9833333333333333, Precision: 0.8676470588235294, F1-Score: 0.9218749999999999\n",
      "File: label_loop10.csv result_df_loop10.csv | Recall: 0.6166666666666667, Precision: 0.8222222222222222, F1-Score: 0.7047619047619048\n",
      "File: label_multi10.csv result_df_multi10.csv | Recall: 0.7333333333333333, Precision: 0.8627450980392157, F1-Score: 0.7927927927927927\n",
      "File: label_pair100.csv result_df_pair100.csv | Recall: 1.0, Precision: 0.8955223880597015, F1-Score: 0.9448818897637796\n",
      "File: label_random100.csv result_df_random100.csv | Recall: 1.0, Precision: 0.8955223880597015, F1-Score: 0.9448818897637796\n",
      "File: label_replace.csv result_df_replace.csv | Recall: 1.0, Precision: 0.8823529411764706, F1-Score: 0.9375\n",
      "File: label_tail100.csv result_df_tail100.csv | Recall: 0.5333333333333333, Precision: 0.8205128205128205, F1-Score: 0.6464646464646464\n"
     ]
    }
   ],
   "source": [
    "def calculate_metrics(label_file, result_file):\n",
    "    # 讀取真實標籤和預測結果\n",
    "    label_df = pd.read_csv(label_file)\n",
    "    result_df = pd.read_csv(result_file)\n",
    "\n",
    "    # 確保is_anomaly欄位是布爾型\n",
    "    label_df['is_anomaly'] = label_df['is_anomaly'].astype(bool)\n",
    "    result_df['is_anomaly'] = result_df['is_anomaly'].astype(bool)\n",
    "\n",
    "    # 合併數據\n",
    "    merged_df = pd.merge(label_df, result_df, on='TraceId', suffixes=('_true', '_pred'))\n",
    "\n",
    "    # 獲取真實標籤和預測結果\n",
    "    y_true = merged_df['is_anomaly_true']\n",
    "    y_pred = merged_df['is_anomaly_pred']\n",
    "\n",
    "    # 計算recall, precision, f1-score\n",
    "    recall = recall_score(y_true, y_pred, pos_label=True)\n",
    "    precision = precision_score(y_true, y_pred, pos_label=True)\n",
    "    f1 = f1_score(y_true, y_pred, pos_label=True)\n",
    "\n",
    "    return recall, precision, f1\n",
    "\n",
    "# 獲取所有label和result文件\n",
    "label_files = [f for f in os.listdir(label_dir) if f.startswith('label_')]\n",
    "result_files = [f for f in os.listdir(result_dir) if f.startswith('result_df_')]\n",
    "\n",
    "\n",
    "label_files.sort()\n",
    "result_files.sort()\n",
    "\n",
    "\n",
    "for label_file, result_file in zip(label_files, result_files):\n",
    "    recall, precision, f1 = calculate_metrics(label_dir + label_file, result_dir + result_file)\n",
    "    print(f\"File: {label_file} {result_file} | Recall: {recall}, Precision: {precision}, F1-Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../dataset/otlp_1209/export_data/exported_data_1209.csv\")\n",
    "\n",
    "if 'Content' in df.columns:\n",
    "    # df['Content'] = df['Content'].str.replace('[^a-zA-Z0-9]', ' ', regex=True).str.lower()\n",
    "    df['Content'] = df['Content'].str.lstrip('/')\n",
    "\n",
    "df.to_csv(\"../dataset/otlp_1209/export_data/exported_data_1209_noslash.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "logbert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
